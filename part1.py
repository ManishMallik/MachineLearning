# -*- coding: utf-8 -*-
"""part1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m4aXhJ0seROjo74kPIq_Wy-Uxerv4BWT

Will need data processing beforehand
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
#correlation analysis
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"
column_names = ["MPG", "Cylinders", "Displacement", "Horsepower", "Weight", "Acceleration", "Model Year", "Origin", "Car Name"]
data = pd.read_csv(url, names=column_names, delim_whitespace=True)
data = data.sample(frac=1)
#data = pd.read_csv("machine.data.txt", sep=",")
print(data)
# Calculate the correlation matrix
correlation_matrix = data.corr()

# Extract correlations with 'MPG' and sort them in descending order
correlations_with_mpg = correlation_matrix["MPG"].sort_values(ascending=False)

# Print the correlations
print(correlations_with_mpg)

"""Choosing the features (we will use 6 features: horsepower and car name not included)"""

X = data[['Cylinders', 'Displacement', 'Weight', 'Acceleration', 'Model Year', 'Origin']]
y = data['MPG']
rows = data.shape[0]
ratio = 0.8
index = int(rows * ratio)
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = X[0:index]
y_train = y[0:index]
X_test = X[index:]
y_test = y[index:]
X_train = np.array(X_train)
y_train = np.array(y_train)
X_test = np.array(X_test)
y_test = np.array(y_test)
# plt.scatter(X_MMIN, y, s=5, label = 'MMIN')
# plt.scatter(X_MMAX, y, s=5, label = 'MMAX')
# # plt.scatter(X_weight, y, s=5, label = 'Weight')
# # plt.scatter(X_accel, y, s=5, label = 'Acceleration')
# plt.legend(fontsize=15)
# plt.xlabel('Weight and Acceleration', fontsize=15)
# plt.ylabel('ERP', fontsize=15)
# plt.legend()
# plt.show()

"""Predict"""

def predict(X, theta):
    #return np.dot(X, theta)
    return np.dot(X, theta[1:]) + theta[0]

"""MSE loss funct"""

def mse_loss_funct(y_pred, y_act):
    # formula
    return np.mean((y_pred - y_act) ** 2)

"""Gradient Decent Function (I think lol)"""

def ssr_gradient(X, y, w):
    # res = w[0] + w[1] * x[0] + w[2] * x[1] + w[3] * x[2] + w[4] * x[3] - y
    m = len(y)
    res = np.dot(X, w) - y
    gradient = 1 / m * (np.dot(X.T, res))
    return gradient

def gradient_descent(
     gradient, x, y, start, learn_rate, n_iter, tolerance=1e-07
 ):
  cost_history = [1e10]
  vector = start
  for _ in range(n_iter):
    # diff = -learn_rate * np.array(gradient(x, y, vector))
    pred = np.dot(x, vector)
    err = pred - y
    cost = 1/(2*len(y)) * np.dot(err.T, err)
    cost_history.append(cost)
    gradient_vector = gradient(x, y, vector)
    diff = -learn_rate * gradient_vector
    if np.all(np.abs(diff) <= tolerance):
      break
    vector += diff
  return vector, cost_history

"""Training"""

learning_rate = 0.0000001
num_iter = 1000000
start=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]

X_adjusted = np.column_stack((np.ones((X_train.shape[0], 1)), X_train))
# print(X_augmented)
weights, costs = gradient_descent(
    ssr_gradient, X_adjusted, y_train, start, learning_rate, num_iter
)

x_val = 55
print(weights)
hyp = weights[0] + weights[1]*X_adjusted[x_val][1] + weights[2]*X_adjusted[x_val][2] + weights[3]*X_adjusted[x_val][3] + weights[4]*X_adjusted[x_val][4] + weights[5]*X_adjusted[x_val][5] + weights[6]*X_adjusted[x_val][6]
print(hyp)
print(y[x_val])

hypothesis_train = predict(X_train, weights)
mse = mse_loss_funct(hypothesis_train, y_train)
print(mse)
mse = mean_squared_error(hypothesis_train, y_train)
print(mse)

print(r2_score(y_train, hypothesis_train))

hypothesis_test = predict(X_test, weights)
mse = mse_loss_funct(hypothesis_test, y_test)
print(mse)
mse = mean_squared_error(hypothesis_test, y_test)
print(mse)

print(r2_score(y_test, hypothesis_test))
print(costs)
x = np.arange(1, len(costs) + 1)
plt.plot(x, costs, "r", scalex=1000, scaley=2)
plt.title('Cost Function J', size = 30 )
plt.xlabel('No. of iterations', size = 20)
plt.ylabel('Cost', size=20)
plt.show()

import matplotlib.pyplot as plt



cols = ['Cylinders', 'Displacement', 'Weight', 'Acceleration', 'Model Year', 'Origin', 'MPG']
X_train_df = pd.DataFrame(X_train, columns=cols[:-1])

# input and output
fig, axs = plt.subplots(1, len(cols) - 1, figsize=(20, 5))


xLabels = cols[:-1]

for i, subp in enumerate(axs):
    xVals = X_train_df[xLabels[i]]
    subp.scatter(xVals, y_train, alpha=0.5)
    subp.set_xlabel(xLabels[i])
    subp.set_ylabel('MPG')

plt.tight_layout()
plt.show()

"""https://www.kaggle.com/code/tentotheminus9/linear-regression-from-scratch-gradient-descent"""